{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "viral_tweets.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrVTpkzVviE4"
      },
      "source": [
        "Viral Tweets \n",
        "\n",
        "In this project, we are going to use the K-Nearest Neighbor (KNN) machine learning algorithm to predict whether a tweet will go viral. Below are the sections of the project for better navigation:\n",
        "\n",
        "\n",
        "1. Libraries\n",
        "2. Inspection of the dataset\n",
        "3. Creating features and labels\n",
        "4. Normalizing the data\n",
        "5. Spliting Train and Test Set\n",
        "6. Training the Classifier\n",
        "7. Fine tuning the parameter 'k'\n",
        "8. Defining best prediction ratio (score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4FaYkzrhFog"
      },
      "source": [
        "##Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HIN4sXnhEK5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1vdzVVJg0g3"
      },
      "source": [
        "## Inspection of the dataset\n",
        "\n",
        "\n",
        "First lets take a look on the dataset to get some insights and inspect the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kqjDWpAviE_",
        "outputId": "f2bdaf48-3a94-41f7-965d-7424f510bcbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### Importing the json file from my github repository into a pandas DataFrame object ###\n",
        "all_tweets = pd.read_json(\"https://raw.githubusercontent.com/Joaoluislins/Twitter_classification/main/random_tweets.json\", lines=True)\n",
        "\n",
        "print(all_tweets.info())\n",
        "\n",
        "\n",
        "#Print the user here and the user's location here.\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11099 entries, 0 to 11098\n",
            "Data columns (total 31 columns):\n",
            " #   Column                     Non-Null Count  Dtype              \n",
            "---  ------                     --------------  -----              \n",
            " 0   created_at                 11099 non-null  datetime64[ns, UTC]\n",
            " 1   id                         11099 non-null  int64              \n",
            " 2   id_str                     11099 non-null  int64              \n",
            " 3   text                       11099 non-null  object             \n",
            " 4   truncated                  11099 non-null  bool               \n",
            " 5   entities                   11099 non-null  object             \n",
            " 6   metadata                   11099 non-null  object             \n",
            " 7   source                     11099 non-null  object             \n",
            " 8   in_reply_to_status_id      1402 non-null   float64            \n",
            " 9   in_reply_to_status_id_str  1402 non-null   float64            \n",
            " 10  in_reply_to_user_id        1503 non-null   float64            \n",
            " 11  in_reply_to_user_id_str    1503 non-null   float64            \n",
            " 12  in_reply_to_screen_name    1503 non-null   object             \n",
            " 13  user                       11099 non-null  object             \n",
            " 14  geo                        17 non-null     object             \n",
            " 15  coordinates                17 non-null     object             \n",
            " 16  place                      156 non-null    object             \n",
            " 17  contributors               0 non-null      float64            \n",
            " 18  retweeted_status           7372 non-null   object             \n",
            " 19  is_quote_status            11099 non-null  bool               \n",
            " 20  retweet_count              11099 non-null  int64              \n",
            " 21  favorite_count             11099 non-null  int64              \n",
            " 22  favorited                  11099 non-null  bool               \n",
            " 23  retweeted                  11099 non-null  bool               \n",
            " 24  lang                       11099 non-null  object             \n",
            " 25  possibly_sensitive         3192 non-null   float64            \n",
            " 26  quoted_status_id           1154 non-null   float64            \n",
            " 27  quoted_status_id_str       1154 non-null   float64            \n",
            " 28  extended_entities          1199 non-null   object             \n",
            " 29  quoted_status              327 non-null    object             \n",
            " 30  withheld_in_countries      2 non-null      object             \n",
            "dtypes: bool(4), datetime64[ns, UTC](1), float64(8), int64(4), object(14)\n",
            "memory usage: 2.3+ MB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsmO1lKUviFb"
      },
      "source": [
        "# Defining Viral Tweets\n",
        "\n",
        "A K-Nearest Neighbor classifier is a supervised machine learning algorithm, and as a result, we need to have a dataset with tagged labels. For this specific example, we need a dataset where every tweet is marked as viral or not viral. Unfortunately, this isn't a feature of our dataset &mdash; we'll need to make it ourselves.\n",
        "\n",
        "So how do we define a viral tweet? A good place to start is to look at the number of retweets the tweet has. This can be found using the feature `\"retweet_count\"`. Let's say we wanted to create a column called `is_viral` that is a `1` if the tweet had more than `5` retweets and `0` otherwise. We could do that like this:\n",
        "\n",
        "```py\n",
        "all_tweets['is_viral'] = np.where(all_tweets['retweet_count'] > 5, 1, 0)\n",
        "```\n",
        "\n",
        "Instead of using `5` as the benchmark for a viral tweet, let's use the median number of retweets. You can find that by calling the `median()` function on `all_tweets[\"retweet_count\"]`. Print the median number of retweets to understand what this threshold is.\n",
        "\n",
        "Print the number of viral tweets and non-viral tweets. You can do this using `all_tweets['is_viral'].value_counts()`.\n",
        "\n",
        "After finishing this project, consider coming back and playing with this threshold number. How do you think your model would work if it was trying to find incredibly viral tweets? For example, how would it work if it were looking for tweets with 1000 or more retweets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1YKME3cviFd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ATNDwusviFf"
      },
      "source": [
        "# Making Features\n",
        "\n",
        "Now that we've created a label for every tweet in our dataset, we can begin thinking about which features might determine whether a tweet is viral. We can create new columns in our dataset to represent these features. For example, let's say we think the length of a tweet might be a valuable feature. The following line creates a new column containing the length of the tweet.\n",
        "\n",
        "```py\n",
        "all_tweets['tweet_length'] = all_tweets.apply(lambda tweet: len(tweet['text']), axis=1)\n",
        "```\n",
        "\n",
        "Setting `axis = 1` creates a new column rather than a new row.\n",
        "\n",
        "Create a new column called `followers_count` that contains the number of followers of each user. You can find this information in `tweet['user']['followers_count']`. Do the same for `friends_count`.\n",
        "\n",
        "For the rest of this project, we will be using these three features, but we encourage you to create your own. Here are some potential ideas for more features.\n",
        "\n",
        "* The number of hashtags in the tweet. You can find this by looking at the `text` of the tweet and using the `.count()` function with `#` as a parameter.\n",
        "* The number of links in the tweet. Using a similar strategy to the one above, use `.count()` to count the number of times `http` appears in the tweet.\n",
        "* The number of words in the tweet. Call `.split()` on the `text` of a tweet. This will give you a list of the words in the tweet. Find the length of that list.\n",
        "* The average length of the words in the tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BybeZjufviFi"
      },
      "source": [
        "all_tweets['tweet_length'] = all_tweets.apply(lambda tweet: len(tweet['text']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btCH2NB8viFv"
      },
      "source": [
        "# Normalizing The Data\n",
        "\n",
        "We've now made the columns that we want to feed into our classifier. Let's get rid of all the data that is no longer relevant. Create a variable named `labels` and set it equal to the `'is_viral'` column of all_tweets.\n",
        "\n",
        "If we had a dataframe named `df` we could get a single column named `A` like this:\n",
        "\n",
        "```py\n",
        "one_column = df['A']\n",
        "```\n",
        "\n",
        "Create a variable named `data` and set it equal to all of the columns that you created in the last step. Those columns are `tweet_length`, `followers_count`, and `friends_count`.\n",
        "\n",
        "When selecting multiple columns, the names of the columns should be in a list. Check out the example below to see how to select column `A` *and* `B`:\n",
        "\n",
        "```py\n",
        "features = df[['A', 'B']]\n",
        "```\n",
        "\n",
        "Now create a new variable named `scaled_data`. `scaled_data` should be the result of the `scale` function with `data` as a parameter. Also include the parameter `axis = 0`. This scales the *columns* as opposed to the rows.\n",
        "\n",
        "The scale function will normalize the data so all of the features will vary within the same range.\n",
        "\n",
        "Print `scaled_data[0]` to get a sense of what our data looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTVH21O1viF2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btaKAbn3viF3"
      },
      "source": [
        "# Creating the Training Set and Test Set\n",
        "\n",
        "To evaluate the effectiveness of our classifier, we now split `scaled_data` and `labels` into a training set and test set using scikit-learn's `train_test_split` function. This function takes two required parameters: It takes the data, followed by the labels. Set the optional parameter `test_size` to be `0.2`. You can also set the `random_state` parameter so your code will randomly split the data in the same way as our solution code splits the data. We used `random_state = 1`. Remember, this function returns 4 items in this order:\n",
        "\n",
        "1. The training data\n",
        "2. The testing data\n",
        "3. The training labels\n",
        "4. The testing labels\n",
        "\n",
        "Store the results in variables named `train_data`, `test_data`, `train_labels`, and `test_labels`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWVAbEN3viF5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2kl-sBhviF5"
      },
      "source": [
        "# Using the Classifier\n",
        "\n",
        "We can finally use the K-Nearest Neighbor classifier. Let's test it using `k = 5`. Begin by creating a `KNeighborsClassifier` object named `classifier` with the parameter `n_neighbors` equal to `5`.\n",
        "\n",
        "Next, train `classifier` by calling the `.fit()` method with `train_data` and `train_labels` as parameters.\n",
        "\n",
        "Finally, let's test the model! Call `classifier`'s `.score()` method using `test_data` and `test_labels` as parameters. Print the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULNov7s6viF6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OFAkpn-viF7"
      },
      "source": [
        "# Choosing K\n",
        "\n",
        "We've tested our classifier with `k = 5`, but maybe there's a `k` that will work better. Let's test many different values for `k` and graph the results. \n",
        "\n",
        "First, create an empty list called `scores`. Next, create a for loop that has a variable `k` that begins at `1` and ends at `200`.\n",
        "\n",
        "Inside the for loop, create a `KNeighobrsClassifier` object named `classifier` with the parameter `n_neighbors` equal to `k`.\n",
        "\n",
        "Train `classifier` by calling the `.fit()` method with `train_data` and `train_labels` as parameters.\n",
        "\n",
        "Next, let's test the model! Call `classifier`'s `.score()` method using `test_data` and `test_labels` as parameters. `append` the result to `scores`.\n",
        "\n",
        "Finally, let's plot the results. Outside of the loop, use Matplotlib's `plot()` function. `plot()` takes two parameters &mdash; the data on the x-axis and the data on the y-axis. Data on the x-axis should be the values we used for `k`. In this case, `range(1,200)`.  Data on the y-axis should be `scores`. Make sure to call the `plt.show()` function after calling `plt.plot()`. This should take a couple of seconds to run!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GApOUoOviF9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY4TI3SGviF-"
      },
      "source": [
        "# Explore on your own\n",
        "\n",
        "Nice work! You can see the classifier gets better as `k` increases, but as `k` gets too high, underfitting starts to happen.\n",
        "\n",
        "By using the features `tweet_length`, `followers_count`, and `friends_count`, we were able to get up to around 63% accuracy. That is better than random, but still not exceptional. Can you find some different features that perform better? Share your graphs with us on Twitter and maybe it will go viral!"
      ]
    }
  ]
}